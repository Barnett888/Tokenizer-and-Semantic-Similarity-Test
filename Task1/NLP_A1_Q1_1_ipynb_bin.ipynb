{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWCOE6oSRFE4"
      },
      "source": [
        "## File Input\n",
        "\n",
        "First import the files. To run this file, please make the full_contract_txt folder and the path in the following code the same. We define a list named content. Where content[i] refers the the $i^{th}$ file in the folder fulll_contract_txt."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KiLtXa8gSXh",
        "outputId": "51251cea-26f2-42b5-c5bb-3a0a68b8bec1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Z6xSjoIbQbju"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import spacy\n",
        "# pip install spacy\n",
        "# python -m spacy download en_core_web_sm\n",
        "import nltk\n",
        "# pip install nltk\n",
        "import collections\n",
        "\n",
        "# Input folder Path, please modify it when running this file.\n",
        "path = \"/content/drive/MyDrive/ipynb/nlp_a1/full_contract_txt\"\n",
        "os.chdir(path)\n",
        "content=[]\n",
        "def read_text_file(file_path):\n",
        "    with open(file_path, 'r',encoding='utf-8') as f:\n",
        "        content.append(f.read())\n",
        "for file in os.listdir():\n",
        "    if file.endswith(\".txt\"):\n",
        "        file_path = f\"{file}\"\n",
        "        read_text_file(file_path)\n",
        "# May need to modify the path of stop words txt file.\n",
        "path=os.getcwd()\n",
        "path=path.replace(\"/full_contract_txt\",\"\")\n",
        "os.chdir(path)\n",
        "with open(\"stop_words.txt\", 'r', encoding='utf-8') as f:\n",
        "    stopword=f.read()\n",
        "stopword = re.findall(r'\\w+', stopword)\n",
        "# The output path for output and token txt files. May need to modify the path.\n",
        "path=os.getcwd()\n",
        "os.chdir(path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mc_vv2qPTItf"
      },
      "source": [
        "## Define the Tokenizer\n",
        "\n",
        "I programmed a self-defined tokenizer using the re library. The re.findall function can find all parts in a string that satisfy a pattern, and then return all found parts in a list. \\\\\n",
        "we decided to use a self-defined tokenizer because other tokenizers may seperate date, time, multiple symbols like \"!!!!!\", or decimal numbers like 5985.2 into multiple tokens. But we wish them to be in one token. \\\\\n",
        "Here we Briefly explain the meaning of the pattern string: \\\\\n",
        "\n",
        "\n",
        "*   Enter r' ' to start a pattern string. \\\\\n",
        "*   \"\\w\" refers to words, A to Z, a to z, 0 - 9, and _.\n",
        "*   \"\\s\" refers to spaces.\n",
        "*   \"\\[\\]\" combines logics together. Users can also enter exact value in the bracket. Like \\[a-z\\] means a to z, and \\[,.?\\] means only ,.?.\n",
        "*   \"^\" refers to not. So \\[^\\w\\s\\] will pick terms not words and space, therefore will only pick symbols like ^@$_+*/#%!.\n",
        "*   \"+\" means multiple. For example, \\w will only pick one letter(\"a book\" will produce \"\\['a','b','o','o','k'\\]\"), and \\w+ will pick a word(\"\\['a','book'\\]\").\n",
        "*   \"|\" means \"or\" can separate patterns, so a string satisfied any one of these patterns will be picked to the list.\n",
        "\n",
        "Note that if the pattern in the left is satisfied, then Python will not check the following patterns. For example, if we have a pattern \"r'\\w|\\w+\\' \" Then the input \"a book\" will produce \"\\['a','b','o','o','k'\\]\" instead of \"\\['a','book'\\]\".\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7FDCQyNwTR30",
        "outputId": "56adf65f-a822-4e71-8252-931468706fe3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "The original text is: \n",
            " 2015/02/21, I moved to New York. Let's go! It's one-way. I spent $5985.2!!!!!!!!\n",
            "\n",
            "The tokenizer used in the corpus would produce:\n",
            " ['2015/02/21', ',', 'i', 'moved', 'to', 'new', 'york', '.', 'let', \"'\", 's', 'go', '!', 'it', \"'\", 's', 'one', '-', 'way', '.', 'i', 'spent', '$', '5985.2', '!!!!!!!!']\n"
          ]
        }
      ],
      "source": [
        "test_text=\"2015/02/21, I moved to New York. Let's go! It's one-way. I spent $5985.2!!!!!!!!\"\n",
        "print(\"\\nThe original text is: \\n\",test_text)\n",
        "\n",
        "# The main part of the tokenizer\n",
        "pattern=r'\\d+[^\\w\\s]+\\d+[^\\w\\s]+\\d+|\\d+[^\\w\\s]+\\d+|\\w+|[^\\w\\s]+'\n",
        "word=re.findall(pattern, test_text)\n",
        "word=[word.lower() for word in word]\n",
        "\n",
        "print(\"\\nThe tokenizer used in the corpus would produce:\\n\",word)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sw4Lj7_SR6Pf"
      },
      "source": [
        "I decided to program a function that can fix contractions and verb tense.\n",
        "Spacy Library has a contraction fixer. In the first loop of the token\\_fixer function, token.lemma\\_ would try to fix the contraction of all words with a '. This function would return a list of decontracted words(Like \\[\"Let's\",'go'\\] becomes \\[\\['Let','us'\\],'go'\\]), the second loop will fix the list problem(Like \\[\\['Let','us'\\],'go'\\] becomes \\['Let','us','go'\\]). The third loop is the stemmer in the NLTK library to fix the verb tense. \\\\\n",
        "It can fix the verb tense and contractions. Uses an input of a tokenized list, it then returns a new list with fixed contractions and verb tense. However, it is $\\textbf{too computationally expensive}$ to fix the corpus in the question(no output for more than 15 minutes). Therefore I instead apply this function to a simple text for demonstration. The following code is the token fixer. \\\\"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rB4_Dd4StCB",
        "outputId": "5088a230-0ca4-43eb-8595-e765bd664455"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The tokenizer with the fixing function would produce:\n",
            " ['2015/02/21', ',', 'i', 'move', 'to', 'new', 'york', '.', 'let', 'us', 'go', '!', 'it', 'be', 'one-way', '.', 'i', 'spent', '$', '5985.2', '!!!!!!!!']\n"
          ]
        }
      ],
      "source": [
        "pattern_for_fix=r'\\d+[^\\w\\s]+\\d+[^\\w\\s]+\\d+|\\w+[^\\w\\s]+\\w+|\\w+|[^\\w\\s]+'\n",
        "def token_fixer(tokenlist):\n",
        "    word = []\n",
        "    extend_word = []\n",
        "    root_word = []\n",
        "    for i in range(len(tokenlist)):\n",
        "        if re.match(r'\\w+\\'\\w+', tokenlist[i]):\n",
        "            doc = spacy.load(\"en_core_web_sm\")(tokenlist[i])\n",
        "            word = word.__add__([token.lemma_  for token in doc])\n",
        "        else:\n",
        "            word.append(tokenlist[i])\n",
        "    for i in range(len(word)):\n",
        "        if isinstance(word[i], list):\n",
        "            extend_word = extend_word.__add__(word[i])\n",
        "        else:\n",
        "            extend_word.append(word[i])\n",
        "    for i in range(len(extend_word)):\n",
        "        root_word.append(nltk.PorterStemmer().stem(extend_word[i]))\n",
        "        #root_word.append(nltk.WordNetLemmatizer().lemmatize(extend_word[i]))\n",
        "    return root_word\n",
        "\n",
        "# The use of the tokenizer with the decontract and lemma function.\n",
        "word=re.findall(pattern_for_fix, test_text)\n",
        "root_word=token_fixer(word)\n",
        "print(\"The tokenizer with the fixing function would produce:\\n\",root_word)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYGQ_MtGc9YQ"
      },
      "source": [
        "## Solve the Quetion 1\n",
        "\n",
        "# 1 (a)\n",
        "Now apply the tokenizer to the full contract. The list token_full contains all numbers, words and symbols in the folder full_contract_txt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "sWoHkCJPY_oQ"
      },
      "outputs": [],
      "source": [
        "token_words=[]\n",
        "token_full=[]\n",
        "for i in range(len(content)):\n",
        "    tokens = re.findall(pattern, content[i])\n",
        "    #tokens = re.findall(pattern_for_fix, content[i])\n",
        "    token_full = token_full.__add__(tokens)\n",
        "#token_full=token_fix(token_full)\n",
        "token_full=[word.lower() for word in token_full]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZQsNpSIeynp"
      },
      "source": [
        "As the question required, write the result to a file called :\"output.txt\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "6LB9xehIfF2B"
      },
      "outputs": [],
      "source": [
        "\n",
        "with open(\"output.txt\", 'w', encoding='utf-8') as f:\n",
        "    f.write(f\"{token_full}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_itVzS_ofFQh"
      },
      "source": [
        "# 1 (b)\n",
        "Here we used the collection library. It can accept a list input, and output a counter object that shows the frequency of every item in the input list. For example, if we have a list \"\\[1,2,1,2,1,2,2,1,1,3,2,1,2,3,3,4,5,5\\]\", Then the collections.Counter function would produce a Counter\"(\\{1: 6, 2: 6, 3: 3, 5: 2, 4: 1\\})\" . The length of the Counter object is the unique elements in the input list, which is 5 in the exapmle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9n9gr3_QguI6",
        "outputId": "eb746b1d-0fb3-4ec2-e413-4bcebb297c53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The total number of tokens is: 4730682\n",
            "The total number of unique tokens is: 38413\n",
            "The type/token ratio of the corpus is: 0.008119970862552164\n"
          ]
        }
      ],
      "source": [
        "frequency=collections.Counter(token_full)\n",
        "length_token=len(token_full)\n",
        "unique_token=len(frequency)\n",
        "print(\"The total number of tokens is:\", length_token)\n",
        "print(\"The total number of unique tokens is:\", unique_token)\n",
        "print(\"The type/token ratio of the corpus is:\", unique_token/length_token)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgw83QKzgpSC"
      },
      "source": [
        "# 1 (c)\n",
        "As the question required, write the result to a file called :\"token.txt\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "C2fhe1e_jCOL"
      },
      "outputs": [],
      "source": [
        "with open(\"token.txt\", 'w', encoding='utf-8') as f:\n",
        "    f.write(f\"{frequency}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K45i2c2wjPO7"
      },
      "source": [
        "# 1 (d)\n",
        "Now apply a loop that counts the number of tokens in the counter object that have the frequency = 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8HHK6cwjPjq",
        "outputId": "bf589112-0b1c-478c-87a1-ea62ceb2d9ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14437 tokens appeared only once in the corpus.\n"
          ]
        }
      ],
      "source": [
        "token_once=0\n",
        "for word in frequency:\n",
        "    if frequency[f\"{word}\"]==1:\n",
        "        token_once+=1\n",
        "print(f\"{token_once} tokens appeared only once in the corpus.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxqI0rmP04KU"
      },
      "source": [
        "# 1 (e)\n",
        "Intuitively, we should not consider \"_\" or numbers to be a word. But python treats them as a word. So apply the re.search function here which returns TRUE if there is at least one a to z letter in the string. The variable token_words is the tokenizer output with only words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2ppZTh11JTr",
        "outputId": "3105e733-c97e-41cb-ff08-308c2a1d1243"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The total number of words is: 3920618\n",
            "The total number of unique words is: 27721\n",
            "The lexical diversity(type/word ratio) of the corpus is: 0.007070568976625623\n"
          ]
        }
      ],
      "source": [
        "token_words = [word for word in token_full if re.search(r'[a-z]+', word)]\n",
        "frequency_word=collections.Counter(token_words)\n",
        "length_token_word=len(token_words)\n",
        "unique_token_word=len(frequency_word)\n",
        "print(\"The total number of words is:\", length_token_word)\n",
        "print(\"The total number of unique words is:\", unique_token_word)\n",
        "print(\"The lexical diversity(type/word ratio) of the corpus is:\", unique_token_word/length_token_word)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_6ZoiAPUgH8"
      },
      "source": [
        "# 1 (f)\n",
        "When reading the file \"stop_words.txt\", there is one word per line. So we apply the re.findall function here again with a pattern r'\\w+'. This means placing all words into a list with one word per element. This produced the stopword list. Then we need to produce a new list token_without_stopword that contains all words in the list token_word but not in the list stopword. I initially programmed a loop to finish this step. But when I tried the filter with lambda function method that studied online, the compute speed increased. \\\\\n",
        "The filter function can accept a function and a list as input, produce a filter object output. It will run the input function with every element of the input list. If the input function can produce True or False, then only elements with True value would be saved. If the input function produces other output, then the filter function would keep all elements in the input list. Here the lambda function is a quick version to define a function. It will return True if the item is in the list stopword and False otherwise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSQXXgOBUyjV",
        "outputId": "0f8f3873-2b4a-4522-a384-e66636d801d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the top 20 most frequent words and their frequencies are: [('agreement', 43655), ('party', 33277), ('parties', 13523), ('section', 13350), ('company', 12638), ('information', 10943), ('product', 10923), ('date', 10181), ('products', 8201), ('rights', 8067), ('services', 7890), ('applicable', 7540), ('business', 7343), ('set', 7058), ('confidential', 6916), ('written', 6818), ('terms', 6714), ('right', 6681), ('term', 6676), ('notice', 6660)]\n",
            "The lexical diversity(type/word ratio) of the corpus is: 0.014535317221483754\n"
          ]
        }
      ],
      "source": [
        "\n",
        "token_without_stopwords = list(filter(lambda item: item not in stopword, token_words))\n",
        "frequency_without_stopwords=collections.Counter(token_without_stopwords)\n",
        "length_token_without_stopwords=len(token_without_stopwords)\n",
        "unique_token_word_stopwords=len(frequency_without_stopwords)\n",
        "print(\"the top 20 most frequent words and their frequencies are:\",frequency_without_stopwords.most_common(20))\n",
        "print(\"The lexical diversity(type/word ratio) of the corpus is:\", unique_token_word_stopwords/length_token_without_stopwords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5rEGmfygoV-"
      },
      "source": [
        "# 1 (g)\n",
        "NLTK library has a function called bigrams, it can produce a bigrams object. If the input is a string, it will put all two letters next to each other in one tuple(for example, \"list(nltk.bigrams(\"abcd\"))\" would produce \"\\[('a', 'b'), ('b', 'c'), ('c', 'd')\\]\"). If the input is a list, then it will put all two elements next to each other in one tuple(\"list(nltk.bigrams(\\['a','b','c','d'\\]))\" would produce \"\\[('a', 'b'), ('b', 'c'), ('c', 'd')\\]\")."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79r7zLsij8OP",
        "outputId": "0b022b2a-6625-4b40-d6c7-9fcaa2164a27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the top 20 most frequent consecutive words and their frequencies are: [(('confidential', 'information'), 3607), (('intellectual', 'property'), 2936), (('effective', 'date'), 2840), (('written', 'notice'), 2386), (('terms', 'conditions'), 2087), (('set', 'section'), 1826), (('prior', 'written'), 1814), (('term', 'agreement'), 1709), (('confidential', 'treatment'), 1540), (('termination', 'agreement'), 1434), (('parties', 'agree'), 1417), (('securities', 'exchange'), 1410), (('receiving', 'party'), 1368), (('pursuant', 'section'), 1353), (('written', 'consent'), 1330), (('party', 'party'), 1313), (('united', 'states'), 1269), (('applicable', 'law'), 1249), (('disclosing', 'party'), 1210), (('agreement', 'party'), 1206)]\n"
          ]
        }
      ],
      "source": [
        "consecutive_words=list(nltk.bigrams(token_without_stopwords))\n",
        "frequency_consecutive_words=collections.Counter(consecutive_words)\n",
        "print(\"the top 20 most frequent consecutive words and their frequencies are:\",frequency_consecutive_words.most_common(20))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}